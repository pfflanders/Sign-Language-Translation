<!doctype html>
<link rel="stylesheet" href="{{ url_for('static', filename='pic16b_style.css') }}">
<!-- This appears at the top of the browser window -->
<title>PIC16B Project</title>
<nav>
  <!-- Web page heading -->
  <h1>Sign Language Translator</h1>
  <h3>~ Protocol Droids ~</h3>
  <ul>
    <!-- Adds link to submit page on main page -->
    <li><a href="{{ url_for('main') }}">Welcome</a></li>
    <li><a href="{{ url_for('submit') }}">Translate ASL</a></li>
    <li><a href="{{ url_for('index') }}">Live Translation</a></li>
  </ul>
</nav>
<section class="content">
  <header>
    {% block header %}
    <p>Welcome! Feel free to use this American Sign Language (ASL) Parser Tool!</p>{% endblock %}
  </header>
  {% block content %}
    <br>
    <b>Aim of Our Project</b>
    <p>
        Sign languages (or signed languages) are languages that utilize visual and manual modality (i.e., gestures) 
        to convey meaning. Rather like other spoken languages of the world, sign languages are not universal or mutually 
        comprehensible despite the various similarities among them. It is important to note that sign languages are not 
        merely visual manifestations of other spoken languages like English, rather they are unique languages with their 
        own set of linguistic rules. 
    </p>
    <p>
        It's worthwhile to note that our project was originally intended for users who wish to communicate with others 
        via sign language, but the reality is that this is a research project we were mostly interested in seeing 
        if webcams could reasonably parse sign language.  
    </p>
    <p>
        Sign language processing is an emerging field of machine learning that falls within the intersection of natural 
        language processing (NLP) and computer vision. As the name suggests, it involves both the analysis and 
        automatic processing of sign language content. <b>The main goal of this project is to expand the field of sign 
        language translation with a laptop-based tool which provides live-feed translation of basic sign language gestures.</b> 
        The overarching methodology involves creating a Convolutional Neural Network (CNN) that is not only small and 
        fast enough to be run on standard laptops, but also robust enough to accurately translate words and symbols when 
        they are present and not return too many false classifications. 
    </p>
    <p>
        Feel free to look at the github-repo for our project at this 
        <a href="https://github.com/pfflanders/Sign-Language-Translation">link</a>.
    </p>
    <p>
      <b>Limitations of our project</b><br>
      <ul>
        <li>Our translator perhaps not as consistent as we would like.</li>
        <li>Our model is unable to translate letters dependent on gestures such as "Z" or "j"</li>
        <li>ASL relies on gestures and other motions to communicate different words/actions/ meanings, which makes
            our project not a perfect sign language translator, but a decent speller!</li>
      </ul>
      <b>Potential Areas of Improvement</b>
      <ul>
        <li>Given the time, widen the range of sign language input and attempt to process and analyze multiple sign languages</li>
        <li>Fine tune the model to be more consistent with guesses; we would attempt to use a higher image resolution.</li>
        <li>Implement live feed into this webapp.</li>
        <li>Wanted to explore recurrent Neural Networks with LSTM which allows us to have a sense of motion 
          with gestures (data collection with gestures would be extremely difficult).</li>
      </ul>
    </p>
  {% endblock %}
</section>